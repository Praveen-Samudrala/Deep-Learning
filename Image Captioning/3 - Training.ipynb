{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Problem Statement: \n",
    "Annotate an image with a short description explaining the contents in that image.\n",
    "\n",
    "**Dataset used:** \n",
    "The Microsoft **C**ommon **O**bjects in **CO**ntext (MS COCO) dataset is a large-scale dataset for scene understanding.  The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms.  \n",
    "\n",
    "![Sample Dog Output](images/coco-examples.jpg)\n",
    "\n",
    "You can read more about the dataset on the [website](http://cocodataset.org/#home) or in the [research paper](https://arxiv.org/pdf/1405.0312.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The project is divided into the following tasks and, each task is carried out in it's respective Jupyter Notebook.\n",
    " 1. Dataset Exploration: In this notebook, the COCO dataset is explored, in preparation for the project.\n",
    " 2. Preprocessing: In this notebook, COCO dataset is loaded and pre-processed, making it ready to pass to the model for training.\n",
    " 3. **Training: In this notebook, the CNN-RNN deep architecture model is trained.**\n",
    " 4. Inference: In this notebook, the trained model is used to generate captions for images in the test dataset. Here, the performance of the model is observed on real world images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Planning:\n",
    "\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, we will customize the training of the CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 1.1: Necessary Information\n",
    "\n",
    "Let's begin by understanding and setting the following variables that are used during the training process:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  \n",
    "- `save_every` - determines how often to save the model weights.  \n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  \n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "\n",
    "\n",
    "### Chosen CNN-RNN Architecture (ResNet50 - LSTM)\n",
    " \n",
    "- The Encoder consists of CNN network. In our model, we are using pre-trained Resnet50 as our encoder, utilizing all of it's layers except the last output layer as seen from `modules = list(resnet.children())[:-1]` in models.py file.\n",
    "- The Decoder consists of Embedding layer and RNN network. We are using LSTM as RNNs which train on a combined input of image features and caption pairs coming from Embedding layer. LSTM is followed by a Linear layer.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "As this is a Classification task(predicting the next word), I will go for CrossEntropyLoss with Adam Optimizer. Adam optimizer is best for classification tasks over SGD because it has adaptive learning rate and momentum as a part of it's algorithm. Also, with previous experience with Project 1, observed Adam Optimizer was giving lesser loss over SGD. Hence, choosing Adam optimizer with learning rate as 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Constants Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "from workspace_utils import active_session\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 748/414113 [00:00<01:56, 3547.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:38<00:00, 4207.73it/s]\n"
     ]
    }
   ],
   "source": [
    "## Selecting appropriate values for the variables below.\n",
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 7        # minimum word count threshold\n",
    "vocab_from_file = True     # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# Amending the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Building data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initializing the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specifing the learnable parameters of the model.\n",
    "params = list(encoder.embed.parameters()) + list(decoder.parameters())\n",
    "\n",
    "# Defining the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Setting the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7525"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/3236], Loss: 3.6461, Perplexity: 38.3236\n",
      "Epoch [1/3], Step [200/3236], Loss: 3.3239, Perplexity: 27.7690\n",
      "Epoch [1/3], Step [300/3236], Loss: 3.2897, Perplexity: 26.8359\n",
      "Epoch [1/3], Step [400/3236], Loss: 3.3381, Perplexity: 28.1667\n",
      "Epoch [1/3], Step [500/3236], Loss: 3.1367, Perplexity: 23.0279\n",
      "Epoch [1/3], Step [600/3236], Loss: 4.3863, Perplexity: 80.3440\n",
      "Epoch [1/3], Step [700/3236], Loss: 2.8506, Perplexity: 17.2989\n",
      "Epoch [1/3], Step [800/3236], Loss: 3.2354, Perplexity: 25.4160\n",
      "Epoch [1/3], Step [900/3236], Loss: 3.0097, Perplexity: 20.2810\n",
      "Epoch [1/3], Step [1000/3236], Loss: 2.5486, Perplexity: 12.7894\n",
      "Epoch [1/3], Step [1100/3236], Loss: 2.6488, Perplexity: 14.1376\n",
      "Epoch [1/3], Step [1200/3236], Loss: 2.6172, Perplexity: 13.6968\n",
      "Epoch [1/3], Step [1300/3236], Loss: 3.2045, Perplexity: 24.6442\n",
      "Epoch [1/3], Step [1400/3236], Loss: 2.3751, Perplexity: 10.7524\n",
      "Epoch [1/3], Step [1500/3236], Loss: 2.3724, Perplexity: 10.7232\n",
      "Epoch [1/3], Step [1600/3236], Loss: 2.6042, Perplexity: 13.5206\n",
      "Epoch [1/3], Step [1700/3236], Loss: 2.3870, Perplexity: 10.88045\n",
      "Epoch [1/3], Step [1800/3236], Loss: 2.3501, Perplexity: 10.4868\n",
      "Epoch [1/3], Step [1900/3236], Loss: 2.4108, Perplexity: 11.1431\n",
      "Epoch [1/3], Step [2000/3236], Loss: 2.1861, Perplexity: 8.90027\n",
      "Epoch [1/3], Step [2100/3236], Loss: 2.2201, Perplexity: 9.20844\n",
      "Epoch [1/3], Step [2200/3236], Loss: 2.2162, Perplexity: 9.17210\n",
      "Epoch [1/3], Step [2300/3236], Loss: 2.2267, Perplexity: 9.26934\n",
      "Epoch [1/3], Step [2400/3236], Loss: 2.3469, Perplexity: 10.4535\n",
      "Epoch [1/3], Step [2500/3236], Loss: 2.3566, Perplexity: 10.5553\n",
      "Epoch [1/3], Step [2600/3236], Loss: 2.3993, Perplexity: 11.0155\n",
      "Epoch [1/3], Step [2700/3236], Loss: 2.1850, Perplexity: 8.89080\n",
      "Epoch [1/3], Step [2800/3236], Loss: 2.2068, Perplexity: 9.08635\n",
      "Epoch [1/3], Step [2900/3236], Loss: 2.1853, Perplexity: 8.89306\n",
      "Epoch [1/3], Step [3000/3236], Loss: 2.1443, Perplexity: 8.53570\n",
      "Epoch [1/3], Step [3100/3236], Loss: 2.5623, Perplexity: 12.9654\n",
      "Epoch [1/3], Step [3200/3236], Loss: 2.1047, Perplexity: 8.20482\n",
      "Epoch [2/3], Step [100/3236], Loss: 2.3080, Perplexity: 10.05380\n",
      "Epoch [2/3], Step [200/3236], Loss: 2.2542, Perplexity: 9.52736\n",
      "Epoch [2/3], Step [300/3236], Loss: 2.1427, Perplexity: 8.52209\n",
      "Epoch [2/3], Step [400/3236], Loss: 2.3548, Perplexity: 10.5364\n",
      "Epoch [2/3], Step [500/3236], Loss: 2.1640, Perplexity: 8.70636\n",
      "Epoch [2/3], Step [600/3236], Loss: 2.1533, Perplexity: 8.61318\n",
      "Epoch [2/3], Step [700/3236], Loss: 2.0363, Perplexity: 7.66238\n",
      "Epoch [2/3], Step [800/3236], Loss: 2.2857, Perplexity: 9.83298\n",
      "Epoch [2/3], Step [900/3236], Loss: 2.1334, Perplexity: 8.44313\n",
      "Epoch [2/3], Step [1000/3236], Loss: 2.2140, Perplexity: 9.1524\n",
      "Epoch [2/3], Step [1100/3236], Loss: 2.2543, Perplexity: 9.52863\n",
      "Epoch [2/3], Step [1200/3236], Loss: 2.3565, Perplexity: 10.5535\n",
      "Epoch [2/3], Step [1300/3236], Loss: 2.4719, Perplexity: 11.8445\n",
      "Epoch [2/3], Step [1400/3236], Loss: 2.0776, Perplexity: 7.98567\n",
      "Epoch [2/3], Step [1500/3236], Loss: 2.0493, Perplexity: 7.76254\n",
      "Epoch [2/3], Step [1600/3236], Loss: 2.3295, Perplexity: 10.2729\n",
      "Epoch [2/3], Step [1700/3236], Loss: 2.1624, Perplexity: 8.69230\n",
      "Epoch [2/3], Step [1800/3236], Loss: 2.0213, Perplexity: 7.54805\n",
      "Epoch [2/3], Step [1900/3236], Loss: 2.0117, Perplexity: 7.47579\n",
      "Epoch [2/3], Step [2000/3236], Loss: 1.9882, Perplexity: 7.30265\n",
      "Epoch [2/3], Step [2100/3236], Loss: 2.0653, Perplexity: 7.88767\n",
      "Epoch [2/3], Step [2200/3236], Loss: 1.8873, Perplexity: 6.60171\n",
      "Epoch [2/3], Step [2300/3236], Loss: 2.1008, Perplexity: 8.17264\n",
      "Epoch [2/3], Step [2400/3236], Loss: 2.1865, Perplexity: 8.90377\n",
      "Epoch [2/3], Step [2500/3236], Loss: 2.1608, Perplexity: 8.67798\n",
      "Epoch [2/3], Step [2600/3236], Loss: 2.0699, Perplexity: 7.92420\n",
      "Epoch [2/3], Step [2700/3236], Loss: 2.0675, Perplexity: 7.90495\n",
      "Epoch [2/3], Step [2800/3236], Loss: 1.9927, Perplexity: 7.33500\n",
      "Epoch [2/3], Step [2900/3236], Loss: 1.9999, Perplexity: 7.38831\n",
      "Epoch [2/3], Step [3000/3236], Loss: 2.0797, Perplexity: 8.00234\n",
      "Epoch [2/3], Step [3100/3236], Loss: 2.3998, Perplexity: 11.0209\n",
      "Epoch [2/3], Step [3200/3236], Loss: 1.9535, Perplexity: 7.05330\n",
      "Epoch [3/3], Step [100/3236], Loss: 2.4974, Perplexity: 12.15115\n",
      "Epoch [3/3], Step [200/3236], Loss: 2.2056, Perplexity: 9.07565\n",
      "Epoch [3/3], Step [300/3236], Loss: 2.5382, Perplexity: 12.6574\n",
      "Epoch [3/3], Step [400/3236], Loss: 2.0548, Perplexity: 7.80516\n",
      "Epoch [3/3], Step [500/3236], Loss: 1.9749, Perplexity: 7.20554\n",
      "Epoch [3/3], Step [600/3236], Loss: 2.5396, Perplexity: 12.6752\n",
      "Epoch [3/3], Step [700/3236], Loss: 1.8512, Perplexity: 6.36744\n",
      "Epoch [3/3], Step [800/3236], Loss: 2.2102, Perplexity: 9.11735\n",
      "Epoch [3/3], Step [900/3236], Loss: 2.1912, Perplexity: 8.94552\n",
      "Epoch [3/3], Step [1000/3236], Loss: 2.4793, Perplexity: 11.9331\n",
      "Epoch [3/3], Step [1100/3236], Loss: 2.1160, Perplexity: 8.29807\n",
      "Epoch [3/3], Step [1200/3236], Loss: 1.9781, Perplexity: 7.22899\n",
      "Epoch [3/3], Step [1300/3236], Loss: 1.9248, Perplexity: 6.85367\n",
      "Epoch [3/3], Step [1400/3236], Loss: 1.9105, Perplexity: 6.75679\n",
      "Epoch [3/3], Step [1500/3236], Loss: 1.9822, Perplexity: 7.25904\n",
      "Epoch [3/3], Step [1600/3236], Loss: 2.0430, Perplexity: 7.71386\n",
      "Epoch [3/3], Step [1700/3236], Loss: 1.8327, Perplexity: 6.25062\n",
      "Epoch [3/3], Step [1800/3236], Loss: 2.2774, Perplexity: 9.75111\n",
      "Epoch [3/3], Step [1900/3236], Loss: 2.1801, Perplexity: 8.84694\n",
      "Epoch [3/3], Step [2000/3236], Loss: 2.1887, Perplexity: 8.92404\n",
      "Epoch [3/3], Step [2100/3236], Loss: 1.7992, Perplexity: 6.04505\n",
      "Epoch [3/3], Step [2200/3236], Loss: 2.2061, Perplexity: 9.08050\n",
      "Epoch [3/3], Step [2300/3236], Loss: 1.9982, Perplexity: 7.37563\n",
      "Epoch [3/3], Step [2400/3236], Loss: 1.8120, Perplexity: 6.12286\n",
      "Epoch [3/3], Step [2500/3236], Loss: 1.8798, Perplexity: 6.55229\n",
      "Epoch [3/3], Step [2600/3236], Loss: 1.9754, Perplexity: 7.20952\n",
      "Epoch [3/3], Step [2700/3236], Loss: 1.8973, Perplexity: 6.66767\n",
      "Epoch [3/3], Step [2800/3236], Loss: 1.8673, Perplexity: 6.47084\n",
      "Epoch [3/3], Step [2900/3236], Loss: 1.8315, Perplexity: 6.24308\n",
      "Epoch [3/3], Step [3000/3236], Loss: 2.2356, Perplexity: 9.35219\n",
      "Epoch [3/3], Step [3100/3236], Loss: 1.9414, Perplexity: 6.96879\n",
      "Epoch [3/3], Step [3200/3236], Loss: 1.9917, Perplexity: 7.32810\n",
      "Epoch [3/3], Step [3236/3236], Loss: 1.8226, Perplexity: 6.1877"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "with active_session():\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        for i_step in range(1, total_step+1):\n",
    "\n",
    "            # Randomly sample a caption length, and sample indices with that length.\n",
    "            indices = data_loader.dataset.get_train_indices()\n",
    "            \n",
    "            # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch.\n",
    "            images, captions = next(iter(data_loader))\n",
    "\n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Zero the gradients.\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Calculate the batch loss.\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters in the optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get training statistics.\n",
    "            stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "\n",
    "            # Print training statistics (on same line).\n",
    "            print('\\r' + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Print training statistics to file.\n",
    "            f.write(stats + '\\n')\n",
    "            f.flush()\n",
    "\n",
    "            # Print training statistics (on different line).\n",
    "            if i_step % print_every == 0:\n",
    "                print('\\r' + stats)\n",
    "\n",
    "        # Save the weights.\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "            torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
