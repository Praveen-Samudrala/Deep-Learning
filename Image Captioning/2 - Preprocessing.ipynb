{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: \n",
    "Annotate an image with a short description explaining the contents in that image.\n",
    "\n",
    "**Dataset used:** \n",
    "The Microsoft **C**ommon **O**bjects in **CO**ntext (MS COCO) dataset is a large-scale dataset for scene understanding.  The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms.  \n",
    "\n",
    "![Sample Dog Output](images/coco-examples.jpg)\n",
    "\n",
    "You can read more about the dataset on the [website](http://cocodataset.org/#home) or in the [research paper](https://arxiv.org/pdf/1405.0312.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The project is divided into the following tasks and, each task is carried out in it's respective Jupyter Notebook.\n",
    " 1. Dataset Exploration: In this notebook, the COCO dataset is explored, in preparation for the project.\n",
    " 2. **Preprocessing: In this notebook, COCO dataset is loaded and pre-processed, making it ready to pass to the model for training.**\n",
    " 3. Training: In this notebook, the CNN-RNN deep architecture model is trained.\n",
    " 4. Inference: In this notebook, the trained model is used to generate captions for images in the test dataset. Here, the performance of the model is observed on real world images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Planning (Preprocessing):\n",
    "\n",
    "- [Step 1](#step1): Explore the Data Loader\n",
    "- [Step 2](#step2): Use the Data Loader to Obtain Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Explore the Data Loader\n",
    "\n",
    "A data loader is created which can be exlpored in **data_loader.py**. It combines a dataset and a sampler, and provides an iterable over the COCO dataset in batches.\n",
    "\n",
    "\n",
    "The `get_loader` function is called, which takes input a number of arguments. It containes the below parameters to asssign:\n",
    "\n",
    "1. **`transform`** - an [image transform](http://pytorch.org/docs/master/torchvision/transforms.html) specifying how to pre-process the images and convert them to PyTorch tensors before using them as input to the CNN encoder.\n",
    "\n",
    "2. **`mode`** - one of `'train'` (loads the training data in batches) or `'test'` (for the test data). It's used to specify the data loader to be in training or test mode, respectively.  \n",
    "\n",
    "3. **`batch_size`** - determines the batch size.  When training the model, this is number of image-caption pairs used to amend the model weights in each training step.   \n",
    "\n",
    "4. **`vocab_threshold`** - the total number of times that a word must appear in the training captions before it is used as part of the vocabulary.  Words that have fewer than `vocab_threshold` occurrences in the training captions are considered unknown words. \n",
    "\n",
    "5. **`vocab_from_file`** - a Boolean that decides whether to load the vocabulary from file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.2.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "loading annotations into memory...\n",
      "Done (t=1.05s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 937/414113 [00:00<01:27, 4719.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:31<00:00, 4508.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Defining a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Setting the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specifing the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Obtaining the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you ran the code cell above, the data loader was stored in the variable `data_loader`.  \n",
    "\n",
    "You can access the corresponding dataset as `data_loader.dataset`.  This dataset is an instance of the `CoCoDataset` class in **data_loader.py**.\n",
    "\n",
    "### Exploring the `__getitem__` Method\n",
    "\n",
    "The `__getitem__` method in the `CoCoDataset` class determines how an image-caption pair is pre-processed before being incorporated into a batch.  This is true for all `Dataset` classes in PyTorch. \n",
    "\n",
    "When the data loader is in training mode, this method begins by first obtaining the filename (`path`) of a training image and its corresponding caption (`caption`).\n",
    "\n",
    "#### Image Pre-Processing \n",
    "\n",
    "Image pre-processing is relatively straightforward (from the `__getitem__` method in the `CoCoDataset` class):\n",
    "```python\n",
    "# Convert image to tensor and pre-process using transform\n",
    "image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "image = self.transform(image)\n",
    "```\n",
    "After loading the image in the training folder with name `path`, the image is pre-processed using the same transform (`transform_train`) that was supplied when instantiating the data loader.  \n",
    "\n",
    "#### Caption Pre-Processing \n",
    "\n",
    "The captions also need to be pre-processed and prepped for training. For generating captions, we are aiming to create a model that predicts the next token of a sentence from previous tokens, so we turn the caption associated with any image into a list of tokenized words, before casting it to a PyTorch tensor that we can use to train the network.\n",
    "\n",
    "To understand in more detail how COCO captions are pre-processed, we'll first need to take a look at the `vocab` instance variable of the `CoCoDataset` class.  The code snippet below is pulled from the `__init__` method of the `CoCoDataset` class:\n",
    "```python\n",
    "def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        ...\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        ...\n",
    "```\n",
    "From the code snippet above, you can see that `data_loader.dataset.vocab` is an instance of the `Vocabulary` class from **vocabulary.py**.  \n",
    "\n",
    "We use this instance to pre-process the COCO captions (from the `__getitem__` method in the `CoCoDataset` class):\n",
    "\n",
    "```python\n",
    "# Convert caption to tensor of word ids.\n",
    "tokens = nltk.tokenize.word_tokenize(str(caption).lower())   # line 1\n",
    "caption = []                                                 # line 2\n",
    "caption.append(self.vocab(self.vocab.start_word))            # line 3\n",
    "caption.extend([self.vocab(token) for token in tokens])      # line 4\n",
    "caption.append(self.vocab(self.vocab.end_word))              # line 5\n",
    "caption = torch.Tensor(caption).long()                       # line 6\n",
    "```\n",
    "\n",
    "This code converts any string-valued caption to a list of integers, before casting it to a PyTorch tensor.  To see how this code works, let's apply it to the sample caption in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 1`** of the code snippet, every letter in the caption is converted to lowercase, and the [`nltk.tokenize.word_tokenize`](http://www.nltk.org/) function is used to obtain a list of string-valued tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'person', 'doing', 'a', 'trick', 'on', 'a', 'rail', 'while', 'riding', 'a', 'skateboard', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 2`** and **`line 3`** we initialize an empty list and append an integer to mark the start of a caption.  The [paper](https://arxiv.org/pdf/1411.4555.pdf) that you are encouraged to implement uses a special start word (and a special end word, which we'll examine below) to mark the beginning (and end) of a caption.\n",
    "\n",
    "This special start word (`\"<start>\"`) is decided when instantiating the data loader and is passed as a parameter (`start_word`).  You are **required** to keep this parameter at its default value (`start_word=\"<start>\"`).\n",
    "\n",
    "As you will see below, the integer `0` is always used to mark the start of a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special start word: <start>\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sample_caption = []\n",
    "\n",
    "start_word = data_loader.dataset.vocab.start_word\n",
    "print('Special start word:', start_word)\n",
    "sample_caption.append(data_loader.dataset.vocab(start_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 4`**, we continue the list by adding integers that correspond to each of the tokens in the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18]\n"
     ]
    }
   ],
   "source": [
    "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **`line 5`**, we append a final integer to mark the end of the caption.  \n",
    "\n",
    "Identical to the case of the special start word (above), the special end word (`\"<end>\"`) is decided when instantiating the data loader and is passed as a parameter (`end_word`).  You are **required** to keep this parameter at its default value (`end_word=\"<end>\"`).\n",
    "\n",
    "As you will see below, the integer `1` is always used to  mark the end of a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special end word: <end>\n",
      "[0, 3, 98, 754, 3, 396, 39, 3, 1009, 207, 139, 3, 753, 18, 1]\n"
     ]
    }
   ],
   "source": [
    "end_word = data_loader.dataset.vocab.end_word\n",
    "print('Special end word:', end_word)\n",
    "\n",
    "sample_caption.append(data_loader.dataset.vocab(end_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in **`line 6`**, we convert the list of integers to a PyTorch tensor and cast it to [long type](http://pytorch.org/docs/master/tensors.html#torch.Tensor.long).  You can read more about the different types of PyTorch tensors on the [website](http://pytorch.org/docs/master/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     3,    98,   754,     3,   396,    39,     3,  1009,\n",
      "          207,   139,     3,   753,    18,     1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_caption = torch.Tensor(sample_caption).long()\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, any caption is converted to a list of tokens, with _special_ start and end tokens marking the beginning and end of the sentence:\n",
    "```\n",
    "[<start>, 'a', 'person', 'doing', 'a', 'trick', 'while', 'riding', 'a', 'skateboard', '.', <end>]\n",
    "```\n",
    "This list of tokens is then turned into a list of integers, where every distinct word in the vocabulary has an associated integer value:\n",
    "```\n",
    "[0, 3, 98, 754, 3, 396, 207, 139, 3, 753, 18, 1]\n",
    "```\n",
    "Finally, this list is converted to a PyTorch tensor.  All of the captions in the COCO dataset are pre-processed using this same procedure from **`lines 1-6`** described above.  \n",
    "\n",
    "As you saw, in order to convert a token to its corresponding integer, we call `data_loader.dataset.vocab` as a function. \n",
    "\n",
    "```python\n",
    "def __call__(self, word):\n",
    "    if not word in self.word2idx:\n",
    "        return self.word2idx[self.unk_word]\n",
    "    return self.word2idx[word]\n",
    "```\n",
    "\n",
    "The `word2idx` instance variable is a Python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that is indexed by string-valued keys (mostly tokens obtained from training captions).  For each key, the corresponding value is the integer that the token is mapped to in the pre-processing step.\n",
    "\n",
    "Use the code cell below to view a subset of this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 0,\n",
       " '<end>': 1,\n",
       " '<unk>': 2,\n",
       " 'a': 3,\n",
       " 'very': 4,\n",
       " 'clean': 5,\n",
       " 'and': 6,\n",
       " 'well': 7,\n",
       " 'decorated': 8,\n",
       " 'empty': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing the word2idx dictionary.\n",
    "dict(list(data_loader.dataset.vocab.word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', 0),\n",
       " ('<end>', 1),\n",
       " ('<unk>', 2),\n",
       " ('a', 3),\n",
       " ('very', 4),\n",
       " ('clean', 5),\n",
       " ('and', 6),\n",
       " ('well', 7),\n",
       " ('decorated', 8),\n",
       " ('empty', 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_loader.dataset.vocab.word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_loader.dataset.vocab.word2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 8855\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see if you examine the code in **vocabulary.py**, the `word2idx` dictionary is created by looping over the captions in the training dataset.  If a token appears no less than `vocab_threshold` times in the training set, then it is added as a key to the dictionary and assigned a corresponding unique integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.89s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:36<00:00, 4304.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "# Modifing the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in vocabulary: 9955\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a few special keys in the `word2idx` dictionary.  You are already familiar with the special start word (`\"<start>\"`) and special end word (`\"<end>\"`).  There is one more special token, corresponding to unknown words (`\"<unk>\"`).  All tokens that don't appear anywhere in the `word2idx` dictionary are considered unknown words.  In the pre-processing step, any unknown tokens are mapped to the integer `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special unknown word: <unk>\n",
      "All unknown words are mapped to this integer: 2\n"
     ]
    }
   ],
   "source": [
    "unk_word = data_loader.dataset.vocab.unk_word\n",
    "print('Special unknown word:', unk_word)\n",
    "\n",
    "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this for yourself below, by pre-processing the provided nonsense words that never appear in the training captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.dataset.vocab('jfkafejw'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing to mention is the `vocab_from_file` argument that is supplied when creating a data loader.  To understand this argument, note that when you create a new data loader, the vocabulary (`data_loader.dataset.vocab`) is saved as a [pickle](https://docs.python.org/3/library/pickle.html) file in the project folder, with filename `vocab.pkl`.\n",
    "\n",
    "If you are still tweaking the value of the `vocab_threshold` argument, you **must** set `vocab_from_file=False` to have your changes take effect.  \n",
    "\n",
    "But once you are happy with the value that you have chosen for the `vocab_threshold` argument, you need only run the data loader *one more time* with your chosen `vocab_threshold` to save the new vocabulary to file.  Then, you can henceforth set `vocab_from_file=True` to load the vocabulary from file and speed the instantiation of the data loader.  Note that building the vocabulary from scratch is the most time-consuming part of instantiating the data loader, and so you are strongly encouraged to set `vocab_from_file=True` as soon as you are able.\n",
    "\n",
    "Note that if `vocab_from_file=True`, then any supplied argument for `vocab_threshold` when instantiating the data loader is completely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:32<00:00, 4500.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.88s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "# Obtain the data loader (from file). Note that it runs much faster than before!\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Using the Data Loader to Obtain Batches\n",
    "\n",
    "The captions in the dataset vary greatly in length.  You can see this by examining `data_loader.dataset.caption_lengths`, a Python list with one entry for each training caption (where the value stores the length of the corresponding caption).  \n",
    "\n",
    "In the code cell below, we use this list to print the total number of captions in the training data with each length.  As you will see below, the majority of captions have length 10.  Likewise, very short and very long captions are quite rare.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 12, 11, 9, 11]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset.caption_lengths[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 10 --- count: 86334\n",
      "value: 11 --- count: 79948\n",
      "value:  9 --- count: 71934\n",
      "value: 12 --- count: 57637\n",
      "value: 13 --- count: 37645\n",
      "value: 14 --- count: 22335\n",
      "value:  8 --- count: 20771\n",
      "value: 15 --- count: 12841\n",
      "value: 16 --- count:  7729\n",
      "value: 17 --- count:  4842\n",
      "value: 18 --- count:  3104\n",
      "value: 19 --- count:  2014\n",
      "value:  7 --- count:  1597\n",
      "value: 20 --- count:  1451\n",
      "value: 21 --- count:   999\n",
      "value: 22 --- count:   683\n",
      "value: 23 --- count:   534\n",
      "value: 24 --- count:   383\n",
      "value: 25 --- count:   277\n",
      "value: 26 --- count:   215\n",
      "value: 27 --- count:   159\n",
      "value: 28 --- count:   115\n",
      "value: 29 --- count:    86\n",
      "value: 30 --- count:    58\n",
      "value: 31 --- count:    49\n",
      "value: 32 --- count:    44\n",
      "value: 34 --- count:    39\n",
      "value: 37 --- count:    32\n",
      "value: 33 --- count:    31\n",
      "value: 35 --- count:    31\n",
      "value: 36 --- count:    26\n",
      "value: 38 --- count:    18\n",
      "value: 39 --- count:    18\n",
      "value: 43 --- count:    16\n",
      "value: 44 --- count:    16\n",
      "value: 48 --- count:    12\n",
      "value: 45 --- count:    11\n",
      "value: 42 --- count:    10\n",
      "value: 40 --- count:     9\n",
      "value: 49 --- count:     9\n",
      "value: 46 --- count:     9\n",
      "value: 47 --- count:     7\n",
      "value: 50 --- count:     6\n",
      "value: 51 --- count:     6\n",
      "value: 41 --- count:     6\n",
      "value: 52 --- count:     5\n",
      "value: 54 --- count:     3\n",
      "value: 56 --- count:     2\n",
      "value:  6 --- count:     2\n",
      "value: 53 --- count:     2\n",
      "value: 55 --- count:     2\n",
      "value: 57 --- count:     1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tally the total number of training captions with each length.\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(8, 20771), (12, 57637), (11, 79948), (9, 71934), (16, 7729), (10, 86334), (13, 37645), (14, 22335), (17, 4842), (15, 12841), (22, 683), (19, 2014), (18, 3104), (20, 1451), (21, 999), (28, 115), (24, 383), (30, 58), (23, 534), (26, 215), (7, 1597), (25, 277), (27, 159), (34, 39), (29, 86), (32, 44), (31, 49), (33, 31), (35, 31), (37, 32), (36, 26), (38, 18), (45, 11), (43, 16), (50, 6), (44, 16), (48, 12), (42, 10), (40, 9), (51, 6), (39, 18), (49, 9), (41, 6), (56, 2), (47, 7), (46, 9), (52, 5), (54, 3), (6, 2), (53, 2), (55, 2), (57, 1)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate batches of training data, we begin by first sampling a caption length (where the probability that any length is drawn is proportional to the number of captions with that length in the dataset).  Then, we retrieve a batch of size `batch_size` of image-caption pairs, where all captions have the sampled length.  This approach for assembling batches matches the procedure in [this paper](https://arxiv.org/pdf/1502.03044.pdf) and has been shown to be computationally efficient without degrading performance.\n",
    "\n",
    "Run the code cell below to generate a batch.  The `get_train_indices` method in the `CoCoDataset` class first samples a caption length, and then samples `batch_size` indices corresponding to training data points with captions of that length.  These indices are stored below in `indices`.\n",
    "\n",
    "These indices are supplied to the data loader, which then is used to retrieve the corresponding data points.  The pre-processed images and captions in the batch are stored in `images` and `captions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled indices: [276924, 288652, 368806, 320532, 267986, 200185, 96980, 168797, 137426, 160706]\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 11])\n"
     ]
    }
   ],
   "source": [
    "# Randomly saming data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "\n",
    "# Creating and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Obtaining the batch.\n",
    "images, captions = next(iter(data_loader))\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)\n",
    "\n",
    "# Uncomment the lines of code below to print the pre-processed images and captions.\n",
    "# print('images:', images)\n",
    "# print('captions:', captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: tensor([[[[-1.3987, -1.4158, -1.5014,  ..., -1.9809, -1.9295, -1.8610],\n",
      "          [-1.3130, -1.2788, -1.3987,  ..., -1.9124, -1.8268, -1.7754],\n",
      "          [-1.2788, -1.2274, -1.4158,  ..., -1.8268, -1.8610, -1.8097],\n",
      "          ...,\n",
      "          [-1.0390, -0.9534, -0.9877,  ..., -1.6555, -1.6555, -1.6898],\n",
      "          [-0.9705, -0.9534, -1.0048,  ..., -1.6213, -1.6898, -1.6555],\n",
      "          [-1.0219, -1.1589, -1.1075,  ..., -1.6898, -1.7240, -1.6898]],\n",
      "\n",
      "         [[-1.0903, -1.0728, -1.0903,  ..., -1.5280, -1.6155, -1.5455],\n",
      "          [-1.0553, -1.0553, -1.0028,  ..., -1.4930, -1.5805, -1.5280],\n",
      "          [-1.0378, -1.0203, -0.9678,  ..., -1.4580, -1.4930, -1.4405],\n",
      "          ...,\n",
      "          [-1.0378, -0.8452, -0.7402,  ..., -1.4055, -1.4230, -1.4930],\n",
      "          [-0.9328, -0.8978, -0.8102,  ..., -1.4055, -1.4755, -1.4405],\n",
      "          [-1.0028, -1.0553, -1.0728,  ..., -1.3704, -1.4055, -1.3704]],\n",
      "\n",
      "         [[ 0.0779, -0.0092, -0.0267,  ..., -0.6715, -0.5495, -0.3578],\n",
      "          [ 0.0431,  0.0431,  0.1302,  ..., -0.4973, -0.3927, -0.2358],\n",
      "          [-0.0092,  0.0431,  0.1476,  ..., -0.1487, -0.0964,  0.0431],\n",
      "          ...,\n",
      "          [-0.4450, -0.2881, -0.2707,  ..., -0.4450, -0.5844, -0.5147],\n",
      "          [-0.3753, -0.3578, -0.3927,  ..., -0.3578, -0.5495, -0.4275],\n",
      "          [-0.6367, -0.7238, -0.6715,  ..., -0.3753, -0.4973, -0.4275]]],\n",
      "\n",
      "\n",
      "        [[[-1.0219, -1.0562, -1.3644,  ...,  1.8550,  1.8550,  1.8550],\n",
      "          [-1.2274, -1.1760, -1.2788,  ...,  1.6495,  1.6495,  1.6838],\n",
      "          [-0.9877, -1.1932, -1.1932,  ...,  1.5810,  1.5468,  1.5468],\n",
      "          ...,\n",
      "          [-0.5253, -0.4054, -0.4911,  ..., -0.6794, -0.6623, -0.5424],\n",
      "          [-0.4054, -0.2684, -0.2171,  ..., -0.7993, -0.6281, -0.5424],\n",
      "          [-0.6965, -0.5767, -0.5253,  ..., -0.9192, -0.6623, -0.6109]],\n",
      "\n",
      "         [[-0.4426, -0.4251, -0.8452,  ...,  2.2185,  2.2185,  2.2360],\n",
      "          [-0.7927, -0.6527, -0.8277,  ...,  2.0959,  2.1134,  2.1310],\n",
      "          [-0.5476, -0.6527, -0.7577,  ...,  2.0784,  2.0609,  2.0434],\n",
      "          ...,\n",
      "          [-0.2500, -0.0574, -0.0749,  ..., -0.5126, -0.5476, -0.4776],\n",
      "          [-0.1625, -0.0399,  0.1001,  ..., -0.7052, -0.5826, -0.3901],\n",
      "          [-0.3375, -0.3550, -0.1625,  ..., -0.8452, -0.5651, -0.5301]],\n",
      "\n",
      "         [[ 0.0605, -0.2184, -0.7413,  ...,  2.6226,  2.6226,  2.6051],\n",
      "          [-0.6541, -0.6018, -0.7238,  ...,  2.5877,  2.6051,  2.6051],\n",
      "          [-0.3927, -0.5495, -0.6715,  ...,  2.6226,  2.6226,  2.6051],\n",
      "          ...,\n",
      "          [-0.7238, -0.5321, -0.5321,  ..., -0.9853, -1.0201, -0.8633],\n",
      "          [-0.5844, -0.4275, -0.4275,  ..., -1.1247, -0.9504, -0.7761],\n",
      "          [-0.8458, -0.8633, -0.7587,  ..., -1.2641, -0.9504, -0.8458]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5022,  0.4166, -0.2342,  ...,  0.5193,  0.6392,  0.7591],\n",
      "          [ 0.5536,  0.5364, -0.1999,  ...,  0.8276,  1.0844,  1.0502],\n",
      "          [ 0.0056,  0.2111, -0.2171,  ...,  1.4098,  1.1700,  0.7591],\n",
      "          ...,\n",
      "          [ 0.6392,  0.5364,  0.4337,  ..., -0.1314,  0.1597,  0.2453],\n",
      "          [ 0.6563,  0.4851,  0.5193,  ...,  0.2282,  0.2453,  0.2796],\n",
      "          [ 0.6392,  0.4851,  0.5364,  ...,  0.2967,  0.3481,  0.2453]],\n",
      "\n",
      "         [[ 0.2402,  0.1527, -0.2675,  ...,  0.5028,  0.3452,  0.2752],\n",
      "          [ 0.2927,  0.2927, -0.1625,  ...,  0.5728,  0.6078,  0.3452],\n",
      "          [-0.1275,  0.0826, -0.2675,  ...,  0.9580,  0.4853, -0.2150],\n",
      "          ...,\n",
      "          [-0.1275, -0.1450, -0.1275,  ...,  0.0126,  0.3102,  0.3803],\n",
      "          [-0.0574, -0.1800, -0.0049,  ...,  0.3452,  0.3452,  0.3978],\n",
      "          [-0.0749, -0.1275, -0.0049,  ...,  0.3452,  0.4328,  0.3978]],\n",
      "\n",
      "         [[-0.4624, -0.4101, -0.5495,  ...,  0.0605,  0.1825,  0.1825],\n",
      "          [-0.4798, -0.5670, -0.6367,  ...,  0.4091,  0.5485,  0.3219],\n",
      "          [-0.6018, -0.5844, -0.5321,  ...,  0.9145,  0.5485, -0.1312],\n",
      "          ...,\n",
      "          [-0.8807, -0.7936, -0.6715,  ...,  0.2522,  0.6356,  0.6531],\n",
      "          [-0.8633, -0.7238, -0.6018,  ...,  0.4962,  0.5834,  0.6008],\n",
      "          [-0.7587, -0.7587, -0.6715,  ...,  0.5659,  0.5659,  0.5311]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.6495,  1.6324,  1.6667,  ...,  1.5468,  1.3927,  1.6324],\n",
      "          [ 1.2214,  1.5125,  1.7694,  ...,  1.5810,  1.3927,  1.4269],\n",
      "          [ 1.5297,  1.5297,  1.6153,  ...,  0.9303,  0.6563,  0.8447],\n",
      "          ...,\n",
      "          [ 0.8447,  0.3823,  0.7591,  ..., -0.0801, -0.9363, -0.7137],\n",
      "          [-0.1143, -0.0458,  0.2282,  ..., -0.5082,  0.0912,  0.4679],\n",
      "          [ 0.1939, -0.0116, -0.2513,  ..., -0.7479,  0.1426,  0.8276]],\n",
      "\n",
      "         [[ 1.1506,  1.2206,  1.3606,  ...,  0.2227,  0.2927,  0.6078],\n",
      "          [ 0.7479,  1.1155,  1.4657,  ..., -0.4426, -0.4076, -0.2850],\n",
      "          [ 1.0980,  1.1681,  1.3081,  ..., -1.6155, -1.7206, -1.4755],\n",
      "          ...,\n",
      "          [ 0.6954,  0.2227,  0.5728,  ..., -0.1975, -1.0553, -0.8102],\n",
      "          [-0.2675, -0.1975,  0.0651,  ..., -0.6352, -0.0224,  0.3277],\n",
      "          [-0.0399, -0.2500, -0.4951,  ..., -0.9678, -0.1275,  0.5028]],\n",
      "\n",
      "         [[ 0.6356,  0.7402,  1.0191,  ...,  0.4265,  0.4439,  0.7402],\n",
      "          [ 0.2348,  0.5834,  1.0888,  ..., -0.1138, -0.2010, -0.1312],\n",
      "          [ 0.5834,  0.6182,  0.9145,  ..., -1.2293, -1.4559, -1.2816],\n",
      "          ...,\n",
      "          [ 0.3219, -0.2184,  0.1999,  ..., -0.2184, -1.0898, -0.9330],\n",
      "          [-0.6367, -0.6018, -0.3230,  ..., -0.6541, -0.0964,  0.1825],\n",
      "          [-0.3230, -0.5321, -0.7761,  ..., -0.9504, -0.1487,  0.4091]]],\n",
      "\n",
      "\n",
      "        [[[-1.8610, -1.8953, -1.9467,  ..., -0.7137, -0.9192, -0.1314],\n",
      "          [-1.6898, -1.8439, -1.8268,  ..., -0.6452, -1.0048,  0.6392],\n",
      "          [-1.7240, -1.6898, -1.8097,  ..., -1.2445,  0.2111, -0.1486],\n",
      "          ...,\n",
      "          [ 1.2385,  0.5364,  0.9817,  ...,  1.0673,  0.9817,  0.0912],\n",
      "          [ 0.7419,  0.8276,  1.6153,  ...,  1.1015,  1.1529,  0.7591],\n",
      "          [ 0.6563,  1.6324,  1.7009,  ...,  0.9988,  0.9132,  0.8789]],\n",
      "\n",
      "         [[-1.9657, -2.0182, -1.9832,  ..., -0.7752, -0.8803, -0.2325],\n",
      "          [-1.8256, -1.9482, -2.0007,  ..., -0.6702, -0.9853,  0.6254],\n",
      "          [-1.8081, -1.7906, -1.9307,  ..., -1.1954,  0.2402, -0.2325],\n",
      "          ...,\n",
      "          [ 0.5028,  0.0476,  0.2052,  ...,  0.2227,  0.1877, -0.5476],\n",
      "          [ 0.1877,  0.0476,  0.7479,  ...,  0.1702,  0.2402, -0.0749],\n",
      "          [-0.0399,  0.6604,  0.8354,  ...,  0.1176,  0.0126, -0.0224]],\n",
      "\n",
      "         [[-1.6302, -1.6824, -1.7347,  ..., -0.8110, -0.9504, -0.5495],\n",
      "          [-1.5081, -1.6650, -1.6476,  ..., -0.7587, -1.1073,  0.1302],\n",
      "          [-1.5256, -1.5604, -1.5604,  ..., -1.2641, -0.2707, -0.4275],\n",
      "          ...,\n",
      "          [-1.0376, -1.1073, -1.0898,  ..., -1.1421, -1.1073, -1.3513],\n",
      "          [-0.9504, -1.1596, -1.0201,  ..., -1.1421, -1.1421, -1.2119],\n",
      "          [-1.1073, -0.9156, -1.0201,  ..., -1.2119, -1.2990, -1.2990]]],\n",
      "\n",
      "\n",
      "        [[[-1.7412, -1.7583, -1.7754,  ..., -1.1932, -1.1075, -1.1075],\n",
      "          [-1.7069, -1.7412, -1.7583,  ..., -1.1932, -1.1247, -1.1075],\n",
      "          [-1.7412, -1.7583, -1.7583,  ..., -1.1760, -1.1760, -1.1589],\n",
      "          ...,\n",
      "          [-2.0152, -1.9980, -1.9809,  ..., -0.7137, -1.1075, -1.4158],\n",
      "          [-1.9980, -2.0152, -2.0152,  ..., -1.2788, -1.4158, -1.4672],\n",
      "          [-2.0494, -2.0494, -2.0323,  ..., -1.5357, -1.5014, -1.3815]],\n",
      "\n",
      "         [[-1.8957, -1.8957, -1.8957,  ..., -1.3529, -1.3179, -1.3354],\n",
      "          [-1.8606, -1.8782, -1.8782,  ..., -1.3704, -1.3354, -1.3179],\n",
      "          [-1.8957, -1.8782, -1.8957,  ..., -1.3880, -1.3880, -1.3354],\n",
      "          ...,\n",
      "          [-1.5805, -1.5630, -1.5805,  ..., -1.1954, -1.4405, -1.5805],\n",
      "          [-1.5630, -1.5805, -1.5980,  ..., -1.5455, -1.6331, -1.6506],\n",
      "          [-1.5805, -1.5980, -1.6155,  ..., -1.7206, -1.7031, -1.5805]],\n",
      "\n",
      "         [[-1.7696, -1.7696, -1.7696,  ..., -1.3513, -1.3339, -1.3687],\n",
      "          [-1.7347, -1.7522, -1.7347,  ..., -1.3513, -1.3513, -1.3687],\n",
      "          [-1.7870, -1.7696, -1.7696,  ..., -1.3513, -1.4210, -1.4210],\n",
      "          ...,\n",
      "          [-1.1596, -1.1421, -1.1421,  ..., -1.4384, -1.5953, -1.6476],\n",
      "          [-1.1421, -1.1770, -1.1944,  ..., -1.6302, -1.6999, -1.7347],\n",
      "          [-1.1944, -1.2467, -1.2641,  ..., -1.7522, -1.7522, -1.6999]]]])\n",
      "captions: tensor([[    0,    47,   560,     6,    20,   115,   324,    39,   257,\n",
      "            13,     3,   786,   332,    18,     1],\n",
      "        [    0,   250,   130,     3,   335,  1946,  2291,    39,    32,\n",
      "            24,   101,    32,   278,    18,     1],\n",
      "        [    0,     3,    91,  2454,  2774,    21,     3,   389,  2417,\n",
      "           577,    39,     3,  3858,    18,     1],\n",
      "        [    0,     3,   681,    13,     2,   130,  1173,    21,  6272,\n",
      "             6,     3,  2766,   844,    18,     1],\n",
      "        [    0,     3,    91,   360,     6,   285,   111,     3,  1585,\n",
      "           326,   364,   161,     3,   169,     1],\n",
      "        [    0,     3,   861,    28,    21,  3499,    86,     3,   112,\n",
      "             6,    47,  5781,   644,    18,     1],\n",
      "        [    0,     3,  1483,   372,   224,    39,   257,    13,     3,\n",
      "           523,    21,    35,     2,    18,     1],\n",
      "        [    0,     3,   169,    81,   329,    77,     3,  1319,    21,\n",
      "             3,   335,  4210,   604,    18,     1],\n",
      "        [    0,     3,    80,    13,  2359,  2615,   170,    39,   257,\n",
      "            13,     3,  2359,  2615,    18,     1],\n",
      "        [    0,     3,   169,   491,     3,  2354,    34,     3,   925,\n",
      "            13,  1271,   422,  2076,    18,     1]])\n"
     ]
    }
   ],
   "source": [
    "print('images:', images)\n",
    "print('captions:', captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time you run the code cell above (the sampling one), a different caption length is sampled, and a different batch of training data is returned.  Run the code cell multiple times to check this out!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
